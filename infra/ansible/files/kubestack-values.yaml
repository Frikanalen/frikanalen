alertmanager:
  ingress:
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: default-admin-auth@kubernetescrd
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    enabled: true
    hosts:
      - alertmanager.admin.frikanalen.no
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

grafana:
  # Everything here is defaults, except the "auth" stanza
  # we bypass all authentication because it's already behind
  # an authenticating proxy (admin-auth-service)
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    grafana_net:
      url: https://grafana.net
    server:
      domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ .Values.ingress.hosts | first }}{{ else }}''{{ end }}"
    auth:
      disable_login_form: true
    auth.anonymous:
        enabled: true
        # Can't add the org name yet
        # See https://github.com/grafana/grafana/issues/2908
        #org_name: Frikanalen
        org_role: Admin
    auth.basic:
        enabled: false

  ingress:
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: default-admin-auth@kubernetescrd
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    enabled: true
    hosts:
      - grafana.admin.frikanalen.no

prometheus:
  ingress:
    annotations:
      traefik.ingress.kubernetes.io/router.middlewares: default-admin-auth@kubernetescrd
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
    enabled: true
    hosts:
      - prometheus.admin.frikanalen.no

  prometheusSpec:
    # Because this is the only Prometheus instance in the cluster, we don't
    # need to fuss about with filtering on labels which isn't always
    # well-supported in Helm charts.
    serviceMonitorSelectorNilUsesHelmValues: false

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ceph-block
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    additionalScrapeConfigs: |
      - job_name: "zfs-exporter"
        relabel_configs:
          - target_label: "job"
            replacement: zfs-exporter
        static_configs:
        - targets:
          - 192.168.3.59:9134
      - job_name: "static-node-exporter"
        relabel_configs:
          - target_label: "job"
            replacement: node-exporter
        static_configs:
        - targets:
          - 192.168.3.59:9100

# Relabel "instance" so eg. "192.168.3.33:9100" is replaced by "tx1"
prometheus-node-exporter:
  prometheus:
    monitor:
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: instance
